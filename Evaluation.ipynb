{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import json\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw, ImageTk, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from transformers import AutoModelForObjectDetection, TableTransformerForObjectDetection\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxResize(object):\n",
    "    def __init__(self, max_size=800):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        resized_image = image.resize(\n",
    "            (int(round(scale * width)), int(round(scale * height)))\n",
    "        )\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "\n",
    "# Object detection\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == \"no object\":\n",
    "            objects.append(\n",
    "                {\n",
    "                    \"label\": class_label,\n",
    "                    \"score\": float(score),\n",
    "                    \"bbox\": [float(elem) for elem in bbox],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "def fig2img(fig):\n",
    "    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n",
    "    import io\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_detected_tables(img, det_tables, out_path=None):\n",
    "    plt.imshow(img, interpolation=\"lanczos\")\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 20)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for det_table in det_tables:\n",
    "        bbox = det_table[\"bbox\"]\n",
    "\n",
    "        # Extend the bottom edge of the bounding box\n",
    "        extend_height = (bbox[3] - bbox[1]) * 0.05\n",
    "        bbox[3] += extend_height\n",
    "\n",
    "        # Extend the top edge of the bounding box\n",
    "        bbox[1] -= extend_height\n",
    "\n",
    "        if det_table[\"label\"] == \"table\":\n",
    "            facecolor = (1, 0, 0.45)\n",
    "            edgecolor = (1, 0, 0.45)\n",
    "            alpha = 0.3\n",
    "            linewidth = 2\n",
    "            hatch = \"//////\"\n",
    "        elif det_table[\"label\"] == \"table rotated\":\n",
    "            facecolor = (0.95, 0.6, 0.1)\n",
    "            edgecolor = (0.95, 0.6, 0.1)\n",
    "            alpha = 0.3\n",
    "            linewidth = 2\n",
    "            hatch = \"//////\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            bbox[:2],\n",
    "            bbox[2] - bbox[0],\n",
    "            bbox[3] - bbox[1],\n",
    "            linewidth=linewidth,\n",
    "            edgecolor=\"none\",\n",
    "            facecolor=facecolor,\n",
    "            alpha=0.1,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        rect = patches.Rectangle(\n",
    "            bbox[:2],\n",
    "            bbox[2] - bbox[0],\n",
    "            bbox[3] - bbox[1],\n",
    "            linewidth=linewidth,\n",
    "            edgecolor=edgecolor,\n",
    "            facecolor=\"none\",\n",
    "            linestyle=\"-\",\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        rect = patches.Rectangle(\n",
    "            bbox[:2],\n",
    "            bbox[2] - bbox[0],\n",
    "            bbox[3] - bbox[1],\n",
    "            linewidth=0,\n",
    "            edgecolor=edgecolor,\n",
    "            facecolor=\"none\",\n",
    "            linestyle=\"-\",\n",
    "            hatch=hatch,\n",
    "            alpha=0.2,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(\n",
    "            facecolor=(1, 0, 0.45),\n",
    "            edgecolor=(1, 0, 0.45),\n",
    "            label=\"Table\",\n",
    "            hatch=\"//////\",\n",
    "            alpha=0.3,\n",
    "        ),\n",
    "        Patch(\n",
    "            facecolor=(0.95, 0.6, 0.1),\n",
    "            edgecolor=(0.95, 0.6, 0.1),\n",
    "            label=\"Table (rotated)\",\n",
    "            hatch=\"//////\",\n",
    "            alpha=0.3,\n",
    "        ),\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=legend_elements,\n",
    "        bbox_to_anchor=(0.5, -0.02),\n",
    "        loc=\"upper center\",\n",
    "        borderaxespad=0,\n",
    "        fontsize=10,\n",
    "        ncol=2,\n",
    "    )\n",
    "    plt.gcf().set_size_inches(10, 10)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if out_path is not None:\n",
    "        plt.savefig(out_path, bbox_inches=\"tight\", dpi=150)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def objects_to_crops(img, tokens, objects, class_thresholds, padding=10):\n",
    "    \"\"\"\n",
    "    Process the bounding boxes produced by the table detection model into\n",
    "    cropped table images and cropped tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    table_crops = []\n",
    "    for obj in objects:\n",
    "        if obj[\"score\"] < class_thresholds[obj[\"label\"]]:\n",
    "            continue\n",
    "\n",
    "        cropped_table = {}\n",
    "\n",
    "        bbox = obj[\"bbox\"]\n",
    "        bbox = [\n",
    "            bbox[0] - padding,\n",
    "            bbox[1] - padding,\n",
    "            bbox[2] + padding,\n",
    "            bbox[3] + padding,\n",
    "        ]\n",
    "\n",
    "        cropped_img = img.crop(bbox)\n",
    "\n",
    "        table_tokens = [token for token in tokens if iob(token[\"bbox\"], bbox) >= 0.5]\n",
    "        for token in table_tokens:\n",
    "            token[\"bbox\"] = [\n",
    "                token[\"bbox\"][0] - bbox[0],\n",
    "                token[\"bbox\"][1] - bbox[1],\n",
    "                token[\"bbox\"][2] - bbox[0],\n",
    "                token[\"bbox\"][3] - bbox[1],\n",
    "            ]\n",
    "\n",
    "        # If table is predicted to be rotated, rotate cropped image and tokens/words:\n",
    "        if obj[\"label\"] == \"table rotated\":\n",
    "            cropped_img = cropped_img.rotate(270, expand=True)\n",
    "            for token in table_tokens:\n",
    "                bbox = token[\"bbox\"]\n",
    "                bbox = [\n",
    "                    cropped_img.size[0] - bbox[3] - 1,\n",
    "                    bbox[0],\n",
    "                    cropped_img.size[0] - bbox[1] - 1,\n",
    "                    bbox[2],\n",
    "                ]\n",
    "                token[\"bbox\"] = bbox\n",
    "\n",
    "        cropped_table[\"image\"] = cropped_img\n",
    "        cropped_table[\"tokens\"] = table_tokens\n",
    "\n",
    "        table_crops.append(cropped_table)\n",
    "\n",
    "    return table_crops\n",
    "\n",
    "\n",
    "def get_cell_coordinates_by_row(table_data):\n",
    "    # Extract rows and columns\n",
    "    rows = [entry for entry in table_data if entry[\"label\"] == \"table row\"]\n",
    "    columns = [entry for entry in table_data if entry[\"label\"] == \"table column\"]\n",
    "\n",
    "    # Sort rows and columns by their Y and X coordinates, respectively\n",
    "    rows.sort(key=lambda x: x[\"bbox\"][1])\n",
    "    columns.sort(key=lambda x: x[\"bbox\"][0])\n",
    "\n",
    "    # Function to find cell coordinates\n",
    "    def find_cell_coordinates(row, column):\n",
    "        cell_bbox = [\n",
    "            column[\"bbox\"][0],\n",
    "            row[\"bbox\"][1],\n",
    "            column[\"bbox\"][2],\n",
    "            row[\"bbox\"][3],\n",
    "        ]\n",
    "        return cell_bbox\n",
    "\n",
    "    # Generate cell coordinates and count cells in each row\n",
    "    cell_coordinates = []\n",
    "\n",
    "    for row in rows:\n",
    "        row_cells = []\n",
    "        for column in columns:\n",
    "            cell_bbox = find_cell_coordinates(row, column)\n",
    "            row_cells.append({\"column\": column[\"bbox\"], \"cell\": cell_bbox})\n",
    "\n",
    "        # Sort cells in the row by X coordinate\n",
    "        row_cells.sort(key=lambda x: x[\"column\"][0])\n",
    "\n",
    "        # Append row information to cell_coordinates\n",
    "        cell_coordinates.append(\n",
    "            {\"row\": row[\"bbox\"], \"cells\": row_cells, \"cell_count\": len(row_cells)}\n",
    "        )\n",
    "\n",
    "    # Sort rows from top to bottom\n",
    "    cell_coordinates.sort(key=lambda x: x[\"row\"][1])\n",
    "\n",
    "    return cell_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for the transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config:  {0: 'table', 1: 'table rotated'}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-detection\", revision=\"no_timm\"\n",
    ")\n",
    "\n",
    "print(\"Model config: \", model.config.id2label)\n",
    "\n",
    "detection_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(800),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# update id2label to include \"no object\"\n",
    "id2label = model.config.id2label\n",
    "id2label[len(model.config.id2label)] = \"no object\"\n",
    "\n",
    "\n",
    "#  Structure Model\n",
    "structure_model = TableTransformerForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-structure-recognition-v1.1-all\"\n",
    ")\n",
    "structure_model.to(\"cpu\")\n",
    "\n",
    "structure_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(1000),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# update id2label to include \"no object\"\n",
    "structure_id2label = structure_model.config.id2label\n",
    "structure_id2label[len(structure_id2label)] = \"no object\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=6400, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the TinyVGG class\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add BatchNorm here\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add BatchNorm here\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add BatchNorm here\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, out_channels=hidden_units, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add BatchNorm here\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*16*16, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load the model state_dict correctly\n",
    "model_path = \"models/tinyvgg_model.pt\"\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Create the model instance\n",
    "tfc_model = TinyVGG(\n",
    "    input_shape=checkpoint['input_shape'],\n",
    "    hidden_units=checkpoint['hidden_units'],\n",
    "    output_shape=checkpoint['output_shape']\n",
    ")\n",
    "\n",
    "# Load the state_dict into the model\n",
    "tfc_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "tfc_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class Names:  ['false', 'none', 'true']\n"
     ]
    }
   ],
   "source": [
    "# # Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "class_names = [\"none\", \"true\", \"false\"]\n",
    "class_names.sort()\n",
    "print(\"Class Names: \", class_names)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset and correct answers csv path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      img_name        model_answer\n",
      "0  x9v67l6.jpg  model_answer_type2\n",
      "1  ewA8Rpn.jpg  model_answer_type2\n",
      "2  E6MwJ00.jpg  model_answer_type2\n",
      "3  9tuGAkX.jpg  model_answer_type2\n",
      "4  r53EbYv.jpg  model_answer_type2\n",
      "Total number of questions: 10\n",
      "Answers Type 1: [False, False, False, False, False, False, True, True, True, True]\n",
      "Answers Type 2: [True, True, False, False, False, False, False, True, True, True]\n",
      "Evaluation dataset folder: d:\\Projects\\IIST\\AutoEval\\evaluation-data\\phase_1_eval_dataset_enc\\phase_1_eval_dataset\n"
     ]
    }
   ],
   "source": [
    "model_answer_type1_path = \"evaluation-data/model_answer_type1.csv\"\n",
    "model_answer_type2_path = \"evaluation-data/model_answer_type2.csv\"\n",
    "img_model_answer_mapping = \"evaluation-data/img_model_answer_mapping.csv\"\n",
    "\n",
    "TOTAL_QUESTIONS = 10\n",
    "correct_answers_type1 = pd.read_csv(model_answer_type1_path)\n",
    "correct_answers_type2 = pd.read_csv(model_answer_type2_path)\n",
    "img_mappings = pd.read_csv(img_model_answer_mapping)\n",
    "\n",
    "print(img_mappings.head())\n",
    "\n",
    "ANSWERS_TYPE1 = correct_answers_type1[\"Correct Answer\"].tolist()\n",
    "ANSWERS_TYPE2 = correct_answers_type2[\"Correct Answer\"].tolist()\n",
    "\n",
    "print(f\"Total number of questions: {TOTAL_QUESTIONS}\")\n",
    "print(f\"Answers Type 1: {ANSWERS_TYPE1}\")\n",
    "print(f\"Answers Type 2: {ANSWERS_TYPE2}\")\n",
    "\n",
    "# Load the evaluation dataset\n",
    "evaluation_dataset_folder = 'evaluation-data'\n",
    "evaluation_dataset_folder = pathlib.Path(os.getcwd()) / evaluation_dataset_folder / \"phase_1_eval_dataset_enc/phase_1_eval_dataset\"\n",
    "print(f\"Evaluation dataset folder: {evaluation_dataset_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_extraction_and_classify_image(image_path, model):\n",
    "    file_name = image_path.split(\"/\")[-1]\n",
    "    print(\"Extracting data for:\", file_name)\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    pixel_values = detection_transform(image).unsqueeze(0)\n",
    "    pixel_values = pixel_values.to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    objects = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    tokens = []\n",
    "    detection_class_thresholds = {\"table\": 0.5, \"table rotated\": 0.5, \"no object\": 10}\n",
    "\n",
    "    tables_crops = objects_to_crops(\n",
    "        image, tokens, objects, detection_class_thresholds, padding=0\n",
    "    )\n",
    "    if len(tables_crops) == 0:\n",
    "        # print(\"No tables detected\")\n",
    "        raise Exception(\"No tables detected, please provide a clear and straight image of the question paper.\")\n",
    "        return\n",
    "    cropped_table = tables_crops[0][\"image\"].convert(\"RGB\")\n",
    "\n",
    "    pixel_values = structure_transform(cropped_table).unsqueeze(0)\n",
    "    pixel_values = pixel_values.to(\"cpu\")\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = structure_model(pixel_values)\n",
    "\n",
    "    cells = outputs_to_objects(outputs, cropped_table.size, structure_id2label)\n",
    "    \n",
    "    cell_coordinates = get_cell_coordinates_by_row(cells)\n",
    "\n",
    "    # Plotting the cropped cell regions\n",
    "    original_img_np = np.array(cropped_table)\n",
    "\n",
    "    # Extract cell crops\n",
    "    cell_crops = []\n",
    "    for i, row in enumerate(cell_coordinates):\n",
    "        if i == 0 and len(cell_coordinates) > TOTAL_QUESTIONS:  # Skip header if no of rows > total questions, the image might have a header.\n",
    "            continue\n",
    "        last_cell = row[\"cells\"][-1]\n",
    "        cell_x, cell_y, cell_w, cell_h = [int(x) for x in last_cell[\"cell\"]]\n",
    "        cell_crop = original_img_np[cell_y:cell_h, cell_x:cell_w]\n",
    "        cell_crops.append(cell_crop)\n",
    "    \n",
    "    # If more than 10 cell crops, take only the first 10\n",
    "    if len(cell_crops) > TOTAL_QUESTIONS:\n",
    "        cell_crops = cell_crops[:TOTAL_QUESTIONS]\n",
    "    \n",
    "    # Perform classification for each cell crop\n",
    "    predictions = []\n",
    "    for cell_crop in cell_crops:\n",
    "        cell_image = Image.fromarray(cell_crop).convert(\"RGB\")\n",
    "        input_image_tensor = transform(cell_image).unsqueeze(0)  # Add batch dimension\n",
    "        input_image_tensor = input_image_tensor.to(\"cpu\")\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.inference_mode():\n",
    "            output = tfc_model(input_image_tensor)\n",
    "\n",
    "        # Probabilities\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        label = torch.argmax(probs, dim=1).item()\n",
    "        predictions.append(class_names[label])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing images: 100%|███████████████████████████████████████| 645/645 [16:04<00:00,  1.49s/image]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Errored images: evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/04t0i6U.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/0Ld34zC.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/0lnF9tq.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/0qZVJKm.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/17Eh4M5.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/1BDUOhL.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/1CCExTK.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/1f5kc6O.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/2EufWcq.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/2PvIZDE.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/3kYHjN0.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/3pGCYEU.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/3pwsTyE.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/40B8D9P.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/54jROSP.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/55RvBa9.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/5Abx9M2.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/5NGPXWn.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/5nwXIEF.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/7T7pRQW.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/89Qr0VM.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/9FgCGu3.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/AbOkl8A.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/AEvXq0w.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/AwF0CBa.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/b2UruIc.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/BUtmjUM.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/bvW2cMc.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/CgQmkHz.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/CrwqCnM.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/CutQ325.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/d698jqy.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/DMStlYW.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/DSxCXr6.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/eMa8BlA.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/EPA6x5l.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/epimQv0.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/EPXRcep.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/faXTONV.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/FMctx8O.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/FpQD5VW.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/fq5o2TP.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/FVofCFc.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/G1CZP6M.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/GdDku30.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/gdtcR42.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/GidbMC1.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/GiTl4gv.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/gKxPK1Y.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/GUJBXzs.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/gyP6GmY.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/GyyJzOi.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/H2iPb1j.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/hhJrQUf.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/IbBere6.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/IbR3JQ2.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/icWtiVC.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/Icxu8bH.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/IEH3YjW.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/IqSlvss.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/Itmbytc.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/IUmBbJR.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/J6SsjFs.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/JcwfXfG.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/jiooheu.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/JR5NBIN.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/jVAcbJd.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/kaXEAip.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/Kk3EIE8.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/l1ZVgCG.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/lbg7D6C.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/lLhLJNY.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/LSxnasq.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/MBJiE0X.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/MKo064b.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/MmuQQou.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/mrijc85.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/n1jxrs3.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/NDKzZ1k.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/NIDRNKM.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/niEMuqt.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/NUQPQV8.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/nVDIV7v.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/nx4Jtjd.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/O0KTNBq.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/O63hzUh.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/OasYZ7w.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/Onp7BQB.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/PXamf6c.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/q8iJyk0.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/qJ7zBpR.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/qlwikIg.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/rijC15L.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/rIRR2QT.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/RN7zNhk.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/ROdYDt2.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/roxXaVD.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/Sb9xEDW.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/SGX6mEJ.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/sPGI0Yj.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/STcovYo.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/SUT4AWW.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/SWEGOOj.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/SxOFztL.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/tDvfNiE.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/tjERJXl.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/TQUHeNf.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/U1tXBng.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/u20CbaY.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/u9Wer6S.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/uGqNWz5.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/UmIo7nf.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/v10kqqM.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/VJFlqhC.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/VMvseht.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/vxiTFZD.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/VyHTNQ9.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/wCaaOdq.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/wGegiaZ.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/WJnOlzC.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/WjrciTI.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/xaxBugs.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/xicjOgf.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/xPcfWPI.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/xVlxPxC.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/y2wzyA1.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/y50hEHK.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/yuRHqjH.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/YUtXx1Q.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/yyY2D8i.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/z9iMChP.jpg, evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/zWXFXYW.jpg, "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the folder containing images and the CSV file for marks\n",
    "folder_path = pathlib.Path(\n",
    "    \"evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset\"\n",
    ")\n",
    "marks_csv_path = \"evaluation-data/results.csv\"\n",
    "\n",
    "if not os.path.exists(marks_csv_path):\n",
    "    with open(marks_csv_path, \"w\") as f:\n",
    "        f.write(\"img_name,pred_marks\\n\")\n",
    "\n",
    "# Read the existing marks CSV file into a DataFrame\n",
    "marks_df = pd.read_csv(marks_csv_path)\n",
    "\n",
    "# List to hold paths of images that resulted in errors during processing\n",
    "errored_images = []\n",
    "\n",
    "# Iterate through each image in the folder\n",
    "for image_path in tqdm(list(folder_path.glob(\"*.jpg\")), desc=\"Processing images\", unit=\"image\", ncols=100):\n",
    "    image_name = image_path.name\n",
    "    image_path_str = str(image_path).replace(\"\\\\\", \"/\")\n",
    "    # print(f\"Processing image: {image_path_str}\")\n",
    "    \n",
    "    answer_type = img_mappings[img_mappings[\"img_name\"] == image_name][\n",
    "        \"model_answer\"\n",
    "    ].values[0]\n",
    "    ANSWER = ANSWERS_TYPE1 if answer_type == \"model_answer_type1\" else ANSWERS_TYPE2\n",
    "    total_marks = 0\n",
    "    try:\n",
    "        # Perform extraction and classification on the image\n",
    "        predictions = perform_extraction_and_classify_image(image_path_str, model)\n",
    "\n",
    "        # Compare predictions with correct answers and calculate total marks\n",
    "        for i in range(\n",
    "            min(len(predictions), len(ANSWER))\n",
    "        ):  # Use min() to avoid index out of range error\n",
    "            if (\n",
    "                predictions[i].lower() == str(ANSWER[i]).lower()\n",
    "            ):  # Compare ignoring case\n",
    "                total_marks += 1\n",
    "\n",
    "        print(f\"{image_name} scored: {total_marks}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        # Record the image path that caused an error\n",
    "        errored_images.append(image_path_str)\n",
    "        print(f\"ERROR: {image_path_str} - {str(e)}\")\n",
    "\n",
    "    # Append the results (image name and predicted marks) to the marks DataFrame\n",
    "    marks_df = pd.concat(\n",
    "        [\n",
    "            marks_df,\n",
    "            pd.DataFrame({\"img_name\": [image_name], \"pred_marks\": [total_marks]}),\n",
    "        ],\n",
    "        ignore_index=True,\n",
    "    )\n",
    "    \n",
    "    clear_output(wait=True)\n",
    "\n",
    "# Write the updated marks DataFrame back to the CSV file\n",
    "marks_df.to_csv(marks_csv_path, index=False)\n",
    "\n",
    "# Print list of images that caused errors during processing\n",
    "if len(errored_images) > 0:\n",
    "    print(\"\\nErrored images: \", end=\"\")\n",
    "    for err_image in errored_images:\n",
    "        print(err_image, end=\", \")\n",
    "\n",
    "# Save errored_images to a file\n",
    "errored_file = pathlib.Path('evaluation-data/errored_images.json')\n",
    "if not os.path.exists(errored_file):\n",
    "    with open(errored_file, \"w+\") as data_file:\n",
    "        data_file.write(\"{}\")\n",
    "json_data = {\"errors\": errored_images}\n",
    "with open(errored_file, \"w\") as data_file:\n",
    "    json.dump(json_data, data_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Properly add the marks to _`submission.csv`_ from _`results.csv`_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission file updated successfully.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Define paths to the results and submission CSV files\n",
    "results_csv_path = \"evaluation-data/results.csv\"\n",
    "submission_csv_path = \"evaluation-data/submission.csv\"\n",
    "\n",
    "# Read the results CSV file\n",
    "results_df = pd.read_csv(results_csv_path)\n",
    "\n",
    "# Read the submission CSV file\n",
    "submission_df = pd.read_csv(submission_csv_path)\n",
    "\n",
    "# Create a dictionary from the results for quick lookup\n",
    "results_dict = results_df.set_index('img_name')['pred_marks'].to_dict()\n",
    "\n",
    "# Update the submission DataFrame with the corresponding marks\n",
    "submission_df['pred_marks'] = submission_df['img_name'].map(results_dict)\n",
    "\n",
    "# Write the updated submission DataFrame back to the CSV file\n",
    "submission_df.to_csv(submission_csv_path, index=False)\n",
    "\n",
    "print(\"Submission file updated successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['04t0i6U.jpg',\n",
       " '0Ld34zC.jpg',\n",
       " '0lnF9tq.jpg',\n",
       " '0qZVJKm.jpg',\n",
       " '17Eh4M5.jpg',\n",
       " '1BDUOhL.jpg',\n",
       " '1CCExTK.jpg',\n",
       " '1f5kc6O.jpg',\n",
       " '2EufWcq.jpg',\n",
       " '2PvIZDE.jpg',\n",
       " '3kYHjN0.jpg',\n",
       " '3pGCYEU.jpg',\n",
       " '3pwsTyE.jpg',\n",
       " '40B8D9P.jpg',\n",
       " '54jROSP.jpg',\n",
       " '55RvBa9.jpg',\n",
       " '5Abx9M2.jpg',\n",
       " '5NGPXWn.jpg',\n",
       " '5nwXIEF.jpg',\n",
       " '7T7pRQW.jpg',\n",
       " '89Qr0VM.jpg',\n",
       " '9FgCGu3.jpg',\n",
       " 'AbOkl8A.jpg',\n",
       " 'AEvXq0w.jpg',\n",
       " 'AwF0CBa.jpg',\n",
       " 'b2UruIc.jpg',\n",
       " 'BUtmjUM.jpg',\n",
       " 'bvW2cMc.jpg',\n",
       " 'CgQmkHz.jpg',\n",
       " 'CrwqCnM.jpg',\n",
       " 'CutQ325.jpg',\n",
       " 'd698jqy.jpg',\n",
       " 'DMStlYW.jpg',\n",
       " 'DSxCXr6.jpg',\n",
       " 'eMa8BlA.jpg',\n",
       " 'EPA6x5l.jpg',\n",
       " 'epimQv0.jpg',\n",
       " 'EPXRcep.jpg',\n",
       " 'faXTONV.jpg',\n",
       " 'FMctx8O.jpg',\n",
       " 'FpQD5VW.jpg',\n",
       " 'fq5o2TP.jpg',\n",
       " 'FVofCFc.jpg',\n",
       " 'G1CZP6M.jpg',\n",
       " 'GdDku30.jpg',\n",
       " 'gdtcR42.jpg',\n",
       " 'GidbMC1.jpg',\n",
       " 'GiTl4gv.jpg',\n",
       " 'gKxPK1Y.jpg',\n",
       " 'GUJBXzs.jpg',\n",
       " 'gyP6GmY.jpg',\n",
       " 'GyyJzOi.jpg',\n",
       " 'H2iPb1j.jpg',\n",
       " 'hhJrQUf.jpg',\n",
       " 'IbBere6.jpg',\n",
       " 'IbR3JQ2.jpg',\n",
       " 'icWtiVC.jpg',\n",
       " 'Icxu8bH.jpg',\n",
       " 'IEH3YjW.jpg',\n",
       " 'IqSlvss.jpg',\n",
       " 'Itmbytc.jpg',\n",
       " 'IUmBbJR.jpg',\n",
       " 'J6SsjFs.jpg',\n",
       " 'JcwfXfG.jpg',\n",
       " 'jiooheu.jpg',\n",
       " 'JR5NBIN.jpg',\n",
       " 'jVAcbJd.jpg',\n",
       " 'kaXEAip.jpg',\n",
       " 'Kk3EIE8.jpg',\n",
       " 'l1ZVgCG.jpg',\n",
       " 'lbg7D6C.jpg',\n",
       " 'lLhLJNY.jpg',\n",
       " 'LSxnasq.jpg',\n",
       " 'MBJiE0X.jpg',\n",
       " 'MKo064b.jpg',\n",
       " 'MmuQQou.jpg',\n",
       " 'mrijc85.jpg',\n",
       " 'n1jxrs3.jpg',\n",
       " 'NDKzZ1k.jpg',\n",
       " 'NIDRNKM.jpg',\n",
       " 'niEMuqt.jpg',\n",
       " 'NUQPQV8.jpg',\n",
       " 'nVDIV7v.jpg',\n",
       " 'nx4Jtjd.jpg',\n",
       " 'O0KTNBq.jpg',\n",
       " 'O63hzUh.jpg',\n",
       " 'OasYZ7w.jpg',\n",
       " 'Onp7BQB.jpg',\n",
       " 'PXamf6c.jpg',\n",
       " 'q8iJyk0.jpg',\n",
       " 'qJ7zBpR.jpg',\n",
       " 'qlwikIg.jpg',\n",
       " 'rijC15L.jpg',\n",
       " 'rIRR2QT.jpg',\n",
       " 'RN7zNhk.jpg',\n",
       " 'ROdYDt2.jpg',\n",
       " 'roxXaVD.jpg',\n",
       " 'Sb9xEDW.jpg',\n",
       " 'SGX6mEJ.jpg',\n",
       " 'sPGI0Yj.jpg',\n",
       " 'STcovYo.jpg',\n",
       " 'SUT4AWW.jpg',\n",
       " 'SWEGOOj.jpg',\n",
       " 'SxOFztL.jpg',\n",
       " 'tDvfNiE.jpg',\n",
       " 'tjERJXl.jpg',\n",
       " 'TQUHeNf.jpg',\n",
       " 'U1tXBng.jpg',\n",
       " 'u20CbaY.jpg',\n",
       " 'u9Wer6S.jpg',\n",
       " 'uGqNWz5.jpg',\n",
       " 'UmIo7nf.jpg',\n",
       " 'v10kqqM.jpg',\n",
       " 'VJFlqhC.jpg',\n",
       " 'VMvseht.jpg',\n",
       " 'vxiTFZD.jpg',\n",
       " 'VyHTNQ9.jpg',\n",
       " 'wCaaOdq.jpg',\n",
       " 'wGegiaZ.jpg',\n",
       " 'WJnOlzC.jpg',\n",
       " 'WjrciTI.jpg',\n",
       " 'xaxBugs.jpg',\n",
       " 'xicjOgf.jpg',\n",
       " 'xPcfWPI.jpg',\n",
       " 'xVlxPxC.jpg',\n",
       " 'y2wzyA1.jpg',\n",
       " 'y50hEHK.jpg',\n",
       " 'yuRHqjH.jpg',\n",
       " 'YUtXx1Q.jpg',\n",
       " 'yyY2D8i.jpg',\n",
       " 'z9iMChP.jpg',\n",
       " 'zWXFXYW.jpg']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load errored images from json file\n",
    "errored_file = pathlib.Path('evaluation-data/errored_images.json')\n",
    "with open(errored_file, \"r\") as data_file:\n",
    "    errored_images = json.load(data_file).get('errors')\n",
    "\n",
    "for i in range(len(errored_images)):\n",
    "    errored_images[i] = (\n",
    "        errored_images[i]\n",
    "        .strip()\n",
    "        .replace(\"evaluation-data/phase_1_eval_dataset_enc/phase_1_eval_dataset/\", \"\")\n",
    "    )\n",
    "\n",
    "errored_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## _API for evaluation using flask_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " * Serving Flask app '__main__'\n",
      " * Debug mode: off\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: This is a development server. Do not use it in a production deployment. Use a production WSGI server instead.\n",
      " * Running on http://127.0.0.1:5000\n",
      "Press CTRL+C to quit\n",
      "127.0.0.1 - - [18/Jul/2024 06:51:13] \"GET / HTTP/1.1\" 404 -\n",
      "127.0.0.1 - - [18/Jul/2024 06:51:14] \"GET /favicon.ico HTTP/1.1\" 404 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for: uploads\\image.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Jul/2024 06:59:46] \"POST /evaluate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for: uploads\\image.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Jul/2024 07:01:29] \"POST /evaluate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for: uploads\\image.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Jul/2024 07:01:59] \"POST /evaluate HTTP/1.1\" 200 -\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting data for: uploads\\image.jpg\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "127.0.0.1 - - [18/Jul/2024 07:02:26] \"POST /evaluate HTTP/1.1\" 200 -\n"
     ]
    }
   ],
   "source": [
    "from flask import Flask, request, jsonify\n",
    "import os\n",
    "import pandas as pd\n",
    "from werkzeug.utils import secure_filename\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Configure upload folder\n",
    "UPLOAD_FOLDER = 'uploads'\n",
    "app.config['UPLOAD_FOLDER'] = UPLOAD_FOLDER\n",
    "\n",
    "# Ensure the upload folder exists\n",
    "os.makedirs(UPLOAD_FOLDER, exist_ok=True)\n",
    "\n",
    "@app.route('/evaluate', methods=['POST'])\n",
    "def evaluate_paper():\n",
    "    if 'image' not in request.files or 'csv' not in request.files:\n",
    "        return jsonify({'error': 'Missing image or CSV file'}), 400\n",
    "\n",
    "    image_file = request.files['image']\n",
    "    csv_file = request.files['csv']\n",
    "\n",
    "    if image_file.filename == '' or csv_file.filename == '':\n",
    "        return jsonify({'error': 'No selected file'}), 400\n",
    "\n",
    "    if image_file and csv_file:\n",
    "        # Save the uploaded files\n",
    "        image_filename = secure_filename(image_file.filename)\n",
    "        csv_filename = secure_filename(csv_file.filename)\n",
    "        image_path = os.path.join(app.config['UPLOAD_FOLDER'], image_filename)\n",
    "        csv_path = os.path.join(app.config['UPLOAD_FOLDER'], csv_filename)\n",
    "        image_file.save(image_path)\n",
    "        csv_file.save(csv_path)\n",
    "\n",
    "        # Read the CSV file\n",
    "        correct_answers = pd.read_csv(csv_path)['Correct Answer'].tolist()\n",
    "        TOTAL_QUESTIONS = len(correct_answers)\n",
    "\n",
    "        # Perform extraction and classification\n",
    "        try:\n",
    "            predictions = perform_extraction_and_classify_image(image_path, model)\n",
    "            \n",
    "            # Calculate total marks\n",
    "            total_marks = sum(1 for pred, ans in zip(predictions, correct_answers) if pred.lower() == str(ans).lower())\n",
    "\n",
    "            # Clean up uploaded files\n",
    "            os.remove(image_path)\n",
    "            os.remove(csv_path)\n",
    "\n",
    "            return jsonify({\n",
    "                'total_marks': total_marks,\n",
    "                'predictions': predictions,\n",
    "                'correct_answers': correct_answers\n",
    "            })\n",
    "\n",
    "        except Exception as e:\n",
    "            # Clean up uploaded files\n",
    "            os.remove(image_path)\n",
    "            os.remove(csv_path)\n",
    "            return jsonify({'error': str(e)}), 500\n",
    "\n",
    "    return jsonify({'error': 'Invalid file'}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
