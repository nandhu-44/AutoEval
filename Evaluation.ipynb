{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import pathlib\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "import torch\n",
    "from torch import nn\n",
    "from torchvision import transforms\n",
    "from PIL import Image, ImageDraw, ImageTk, ImageFont\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n",
    "from matplotlib.patches import Patch\n",
    "\n",
    "from transformers import AutoModelForObjectDetection, TableTransformerForObjectDetection\n",
    "\n",
    "from IPython.display import clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Definitions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaxResize(object):\n",
    "    def __init__(self, max_size=800):\n",
    "        self.max_size = max_size\n",
    "\n",
    "    def __call__(self, image):\n",
    "        width, height = image.size\n",
    "        current_max_size = max(width, height)\n",
    "        scale = self.max_size / current_max_size\n",
    "        resized_image = image.resize(\n",
    "            (int(round(scale * width)), int(round(scale * height)))\n",
    "        )\n",
    "\n",
    "        return resized_image\n",
    "\n",
    "\n",
    "# for output bounding box post-processing\n",
    "def box_cxcywh_to_xyxy(x):\n",
    "    x_c, y_c, w, h = x.unbind(-1)\n",
    "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h), (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
    "    return torch.stack(b, dim=1)\n",
    "\n",
    "\n",
    "def rescale_bboxes(out_bbox, size):\n",
    "    img_w, img_h = size\n",
    "    b = box_cxcywh_to_xyxy(out_bbox)\n",
    "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
    "    return b\n",
    "\n",
    "\n",
    "# Object detection\n",
    "def outputs_to_objects(outputs, img_size, id2label):\n",
    "    m = outputs.logits.softmax(-1).max(-1)\n",
    "    pred_labels = list(m.indices.detach().cpu().numpy())[0]\n",
    "    pred_scores = list(m.values.detach().cpu().numpy())[0]\n",
    "    pred_bboxes = outputs[\"pred_boxes\"].detach().cpu()[0]\n",
    "    pred_bboxes = [elem.tolist() for elem in rescale_bboxes(pred_bboxes, img_size)]\n",
    "\n",
    "    objects = []\n",
    "    for label, score, bbox in zip(pred_labels, pred_scores, pred_bboxes):\n",
    "        class_label = id2label[int(label)]\n",
    "        if not class_label == \"no object\":\n",
    "            objects.append(\n",
    "                {\n",
    "                    \"label\": class_label,\n",
    "                    \"score\": float(score),\n",
    "                    \"bbox\": [float(elem) for elem in bbox],\n",
    "                }\n",
    "            )\n",
    "\n",
    "    return objects\n",
    "\n",
    "\n",
    "def fig2img(fig):\n",
    "    \"\"\"Convert a Matplotlib figure to a PIL Image and return it\"\"\"\n",
    "    import io\n",
    "\n",
    "    buf = io.BytesIO()\n",
    "    fig.savefig(buf)\n",
    "    buf.seek(0)\n",
    "    img = Image.open(buf)\n",
    "    return img\n",
    "\n",
    "\n",
    "def visualize_detected_tables(img, det_tables, out_path=None):\n",
    "    plt.imshow(img, interpolation=\"lanczos\")\n",
    "    fig = plt.gcf()\n",
    "    fig.set_size_inches(20, 20)\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for det_table in det_tables:\n",
    "        bbox = det_table[\"bbox\"]\n",
    "\n",
    "        # Extend the bottom edge of the bounding box\n",
    "        extend_height = (bbox[3] - bbox[1]) * 0.05\n",
    "        bbox[3] += extend_height\n",
    "\n",
    "        # Extend the top edge of the bounding box\n",
    "        bbox[1] -= extend_height\n",
    "\n",
    "        if det_table[\"label\"] == \"table\":\n",
    "            facecolor = (1, 0, 0.45)\n",
    "            edgecolor = (1, 0, 0.45)\n",
    "            alpha = 0.3\n",
    "            linewidth = 2\n",
    "            hatch = \"//////\"\n",
    "        elif det_table[\"label\"] == \"table rotated\":\n",
    "            facecolor = (0.95, 0.6, 0.1)\n",
    "            edgecolor = (0.95, 0.6, 0.1)\n",
    "            alpha = 0.3\n",
    "            linewidth = 2\n",
    "            hatch = \"//////\"\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "        rect = patches.Rectangle(\n",
    "            bbox[:2],\n",
    "            bbox[2] - bbox[0],\n",
    "            bbox[3] - bbox[1],\n",
    "            linewidth=linewidth,\n",
    "            edgecolor=\"none\",\n",
    "            facecolor=facecolor,\n",
    "            alpha=0.1,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        rect = patches.Rectangle(\n",
    "            bbox[:2],\n",
    "            bbox[2] - bbox[0],\n",
    "            bbox[3] - bbox[1],\n",
    "            linewidth=linewidth,\n",
    "            edgecolor=edgecolor,\n",
    "            facecolor=\"none\",\n",
    "            linestyle=\"-\",\n",
    "            alpha=alpha,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "        rect = patches.Rectangle(\n",
    "            bbox[:2],\n",
    "            bbox[2] - bbox[0],\n",
    "            bbox[3] - bbox[1],\n",
    "            linewidth=0,\n",
    "            edgecolor=edgecolor,\n",
    "            facecolor=\"none\",\n",
    "            linestyle=\"-\",\n",
    "            hatch=hatch,\n",
    "            alpha=0.2,\n",
    "        )\n",
    "        ax.add_patch(rect)\n",
    "\n",
    "    plt.xticks([], [])\n",
    "    plt.yticks([], [])\n",
    "\n",
    "    legend_elements = [\n",
    "        Patch(\n",
    "            facecolor=(1, 0, 0.45),\n",
    "            edgecolor=(1, 0, 0.45),\n",
    "            label=\"Table\",\n",
    "            hatch=\"//////\",\n",
    "            alpha=0.3,\n",
    "        ),\n",
    "        Patch(\n",
    "            facecolor=(0.95, 0.6, 0.1),\n",
    "            edgecolor=(0.95, 0.6, 0.1),\n",
    "            label=\"Table (rotated)\",\n",
    "            hatch=\"//////\",\n",
    "            alpha=0.3,\n",
    "        ),\n",
    "    ]\n",
    "    plt.legend(\n",
    "        handles=legend_elements,\n",
    "        bbox_to_anchor=(0.5, -0.02),\n",
    "        loc=\"upper center\",\n",
    "        borderaxespad=0,\n",
    "        fontsize=10,\n",
    "        ncol=2,\n",
    "    )\n",
    "    plt.gcf().set_size_inches(10, 10)\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    if out_path is not None:\n",
    "        plt.savefig(out_path, bbox_inches=\"tight\", dpi=150)\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def objects_to_crops(img, tokens, objects, class_thresholds, padding=10):\n",
    "    \"\"\"\n",
    "    Process the bounding boxes produced by the table detection model into\n",
    "    cropped table images and cropped tokens.\n",
    "    \"\"\"\n",
    "\n",
    "    table_crops = []\n",
    "    for obj in objects:\n",
    "        if obj[\"score\"] < class_thresholds[obj[\"label\"]]:\n",
    "            continue\n",
    "\n",
    "        cropped_table = {}\n",
    "\n",
    "        bbox = obj[\"bbox\"]\n",
    "        bbox = [\n",
    "            bbox[0] - padding,\n",
    "            bbox[1] - padding,\n",
    "            bbox[2] + padding,\n",
    "            bbox[3] + padding,\n",
    "        ]\n",
    "\n",
    "        cropped_img = img.crop(bbox)\n",
    "\n",
    "        table_tokens = [token for token in tokens if iob(token[\"bbox\"], bbox) >= 0.5]\n",
    "        for token in table_tokens:\n",
    "            token[\"bbox\"] = [\n",
    "                token[\"bbox\"][0] - bbox[0],\n",
    "                token[\"bbox\"][1] - bbox[1],\n",
    "                token[\"bbox\"][2] - bbox[0],\n",
    "                token[\"bbox\"][3] - bbox[1],\n",
    "            ]\n",
    "\n",
    "        # If table is predicted to be rotated, rotate cropped image and tokens/words:\n",
    "        if obj[\"label\"] == \"table rotated\":\n",
    "            cropped_img = cropped_img.rotate(270, expand=True)\n",
    "            for token in table_tokens:\n",
    "                bbox = token[\"bbox\"]\n",
    "                bbox = [\n",
    "                    cropped_img.size[0] - bbox[3] - 1,\n",
    "                    bbox[0],\n",
    "                    cropped_img.size[0] - bbox[1] - 1,\n",
    "                    bbox[2],\n",
    "                ]\n",
    "                token[\"bbox\"] = bbox\n",
    "\n",
    "        cropped_table[\"image\"] = cropped_img\n",
    "        cropped_table[\"tokens\"] = table_tokens\n",
    "\n",
    "        table_crops.append(cropped_table)\n",
    "\n",
    "    return table_crops\n",
    "\n",
    "\n",
    "def get_cell_coordinates_by_row(table_data):\n",
    "    # Extract rows and columns\n",
    "    rows = [entry for entry in table_data if entry[\"label\"] == \"table row\"]\n",
    "    columns = [entry for entry in table_data if entry[\"label\"] == \"table column\"]\n",
    "\n",
    "    # Sort rows and columns by their Y and X coordinates, respectively\n",
    "    rows.sort(key=lambda x: x[\"bbox\"][1])\n",
    "    columns.sort(key=lambda x: x[\"bbox\"][0])\n",
    "\n",
    "    # Function to find cell coordinates\n",
    "    def find_cell_coordinates(row, column):\n",
    "        cell_bbox = [\n",
    "            column[\"bbox\"][0],\n",
    "            row[\"bbox\"][1],\n",
    "            column[\"bbox\"][2],\n",
    "            row[\"bbox\"][3],\n",
    "        ]\n",
    "        return cell_bbox\n",
    "\n",
    "    # Generate cell coordinates and count cells in each row\n",
    "    cell_coordinates = []\n",
    "\n",
    "    for row in rows:\n",
    "        row_cells = []\n",
    "        for column in columns:\n",
    "            cell_bbox = find_cell_coordinates(row, column)\n",
    "            row_cells.append({\"column\": column[\"bbox\"], \"cell\": cell_bbox})\n",
    "\n",
    "        # Sort cells in the row by X coordinate\n",
    "        row_cells.sort(key=lambda x: x[\"column\"][0])\n",
    "\n",
    "        # Append row information to cell_coordinates\n",
    "        cell_coordinates.append(\n",
    "            {\"row\": row[\"bbox\"], \"cells\": row_cells, \"cell_count\": len(row_cells)}\n",
    "        )\n",
    "\n",
    "    # Sort rows from top to bottom\n",
    "    cell_coordinates.sort(key=lambda x: x[\"row\"][1])\n",
    "\n",
    "    return cell_coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration for the transformer models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model config:  {0: 'table', 1: 'table rotated'}\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-transformer-detection\", revision=\"no_timm\"\n",
    ")\n",
    "\n",
    "print(\"Model config: \", model.config.id2label)\n",
    "\n",
    "detection_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(800),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# update id2label to include \"no object\"\n",
    "id2label = model.config.id2label\n",
    "id2label[len(model.config.id2label)] = \"no object\"\n",
    "\n",
    "\n",
    "#  Structure Model\n",
    "structure_model = TableTransformerForObjectDetection.from_pretrained(\n",
    "    \"microsoft/table-structure-recognition-v1.1-all\"\n",
    ")\n",
    "structure_model.to(\"cpu\")\n",
    "\n",
    "structure_transform = transforms.Compose(\n",
    "    [\n",
    "        MaxResize(1000),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225]),\n",
    "    ]\n",
    ")\n",
    "\n",
    "# update id2label to include \"no object\"\n",
    "structure_id2label = structure_model.config.id2label\n",
    "structure_id2label[len(structure_id2label)] = \"no object\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrained model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TinyVGG(\n",
       "  (conv_block_1): Sequential(\n",
       "    (0): Conv2d(3, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv_block_2): Sequential(\n",
       "    (0): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (2): ReLU()\n",
       "    (3): Conv2d(25, 25, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (4): BatchNorm2d(25, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (5): ReLU()\n",
       "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=6400, out_features=3, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define the TinyVGG class\n",
    "class TinyVGG(nn.Module):\n",
    "    def __init__(self, input_shape: int, hidden_units: int, output_shape: int) -> None:\n",
    "        super().__init__()\n",
    "        self.conv_block_1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=input_shape, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add BatchNorm here\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=hidden_units, out_channels=hidden_units, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add BatchNorm here\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.conv_block_2 = nn.Sequential(\n",
    "            nn.Conv2d(hidden_units, hidden_units, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add BatchNorm here\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(hidden_units, out_channels=hidden_units, kernel_size=3, padding=1),\n",
    "            nn.BatchNorm2d(hidden_units),  # Add BatchNorm here\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=hidden_units*16*16, out_features=output_shape)\n",
    "        )\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        x = self.conv_block_1(x)\n",
    "        x = self.conv_block_2(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "# Load the model state_dict correctly\n",
    "model_path = \"models/tinyvgg_model.pt\"\n",
    "checkpoint = torch.load(model_path, map_location=torch.device('cpu'))\n",
    "\n",
    "# Create the model instance\n",
    "tfc_model = TinyVGG(\n",
    "    input_shape=checkpoint['input_shape'],\n",
    "    hidden_units=checkpoint['hidden_units'],\n",
    "    output_shape=checkpoint['output_shape']\n",
    ")\n",
    "\n",
    "# Load the state_dict into the model\n",
    "tfc_model.load_state_dict(checkpoint['model_state_dict'])\n",
    "\n",
    "# Set the model to evaluation mode\n",
    "tfc_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Do a sample prediction from the image.\n",
    "# # Define transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((64, 64)),\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "# # Load and transform an example image\n",
    "# image_path = \"dataset/trimmed/true/true-20240328_145719-cell-5.png\"\n",
    "# image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "\n",
    "# input_image_tensor = transform(image).unsqueeze(0)  # Add batch dimension\n",
    "# tfc_model.to(\"cpu\")\n",
    "# input_image_tensor = input_image_tensor.to(\"cpu\")\n",
    "\n",
    "# # Perform inference\n",
    "# with torch.inference_mode():\n",
    "#     output = tfc_model(input_image_tensor)\n",
    "\n",
    "# # Logits\n",
    "# print(\"Logits: \", output)\n",
    "\n",
    "# # Probabilities\n",
    "# probs = torch.softmax(output, dim=1)\n",
    "# print(\"Probabilities: \", probs)\n",
    "\n",
    "class_names = [\"none\", \"true\", \"false\"]\n",
    "class_names.sort()\n",
    "\n",
    "# # Predicted label\n",
    "# label = torch.argmax(probs, dim=1)\n",
    "# print(\"Predicted Label: \", label)\n",
    "# print(\"Predicted Class: \", class_names[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read dataset and correct answers csv path "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of questions: 10\n",
      "Answers: [False, False, False, False, False, False, True, True, True, True]\n",
      "Evaluation dataset folder: d:\\Projects\\IIST\\AutoEval\\evaluation-data\n"
     ]
    }
   ],
   "source": [
    "# Prompt for the path to the model answers CSV file\n",
    "# correct_answers_csv_path = input(\"Enter the path to the model answers CSV file: \")\n",
    "correct_answers_csv_path = \"data/ModelAnswer.csv\"\n",
    "correct_answers = pd.read_csv(correct_answers_csv_path)\n",
    "\n",
    "# Get total number of questions\n",
    "TOTAL_QUESTIONS = len(correct_answers)\n",
    "ANSWERS = correct_answers[\"Correct Answer\"].tolist()\n",
    "\n",
    "print(f\"Total number of questions: {TOTAL_QUESTIONS}\")\n",
    "print(f\"Answers: {ANSWERS}\")\n",
    "\n",
    "# Prompt for the path to the evaluation data folder\n",
    "evaluation_dataset_folder = 'evaluation-data'\n",
    "\n",
    "# Use the current working directory if running interactively\n",
    "evaluation_dataset_folder = pathlib.Path(os.getcwd()) / evaluation_dataset_folder\n",
    "\n",
    "# Display the total number of questions and the evaluation dataset folder path for verification\n",
    "print(f\"Evaluation dataset folder: {evaluation_dataset_folder}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cell Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def perform_extraction_and_classify_image(image_path, model):\n",
    "    file_name = image_path.split(\"/\")[-1]\n",
    "    file_name_without_extension = file_name.split(\".\")[0]\n",
    "    print(\"Extracting data for:\", file_name)\n",
    "\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    pixel_values = detection_transform(image).unsqueeze(0)\n",
    "    pixel_values = pixel_values.to(\"cpu\")\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(pixel_values)\n",
    "\n",
    "    objects = outputs_to_objects(outputs, image.size, id2label)\n",
    "\n",
    "    tokens = []\n",
    "    detection_class_thresholds = {\"table\": 0.5, \"table rotated\": 0.5, \"no object\": 10}\n",
    "\n",
    "    tables_crops = objects_to_crops(\n",
    "        image, tokens, objects, detection_class_thresholds, padding=0\n",
    "    )\n",
    "    if len(tables_crops) == 0:\n",
    "        print(\"No tables detected\")\n",
    "        return\n",
    "    cropped_table = tables_crops[0][\"image\"].convert(\"RGB\")\n",
    "\n",
    "    pixel_values = structure_transform(cropped_table).unsqueeze(0)\n",
    "    pixel_values = pixel_values.to(\"cpu\")\n",
    "\n",
    "    # forward pass\n",
    "    with torch.no_grad():\n",
    "        outputs = structure_model(pixel_values)\n",
    "\n",
    "    cells = outputs_to_objects(outputs, cropped_table.size, structure_id2label)\n",
    "    \n",
    "    cell_coordinates = get_cell_coordinates_by_row(cells)\n",
    "\n",
    "    # Plotting the cropped cell regions\n",
    "    original_img_np = np.array(cropped_table)\n",
    "\n",
    "    # Extract cell crops\n",
    "    cell_crops = []\n",
    "    for i, row in enumerate(cell_coordinates):\n",
    "        if i == 0 and len(cell_coordinates) > TOTAL_QUESTIONS:  # Skip header if no of rows > total questions, the image might have a header.\n",
    "            continue\n",
    "        last_cell = row[\"cells\"][-1]\n",
    "        cell_x, cell_y, cell_w, cell_h = [int(x) for x in last_cell[\"cell\"]]\n",
    "        cell_crop = original_img_np[cell_y:cell_h, cell_x:cell_w]\n",
    "        cell_crops.append(cell_crop)\n",
    "    \n",
    "    # If more than 10 cell crops, take only the first 10\n",
    "    if len(cell_crops) > TOTAL_QUESTIONS:\n",
    "        cell_crops = cell_crops[:TOTAL_QUESTIONS]\n",
    "    \n",
    "    # Perform classification for each cell crop\n",
    "    predictions = []\n",
    "    for cell_crop in cell_crops:\n",
    "        cell_image = Image.fromarray(cell_crop).convert(\"RGB\")\n",
    "        input_image_tensor = transform(cell_image).unsqueeze(0)  # Add batch dimension\n",
    "        input_image_tensor = input_image_tensor.to(\"cpu\")\n",
    "\n",
    "        # Perform inference\n",
    "        with torch.inference_mode():\n",
    "            output = tfc_model(input_image_tensor)\n",
    "\n",
    "        # Probabilities\n",
    "        probs = torch.softmax(output, dim=1)\n",
    "        label = torch.argmax(probs, dim=1).item()\n",
    "        predictions.append(class_names[label])\n",
    "    \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Runner Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing image: data/Sample_Data/20240328_145148.jpg\n",
      "Extracting data for: 20240328_145148.jpg\n",
      "20240328_145148.jpg scored: 7\n",
      "Processing image: data/Sample_Data/20240328_145219.jpg\n",
      "Extracting data for: 20240328_145219.jpg\n",
      "20240328_145219.jpg scored: 4\n",
      "Processing image: data/Sample_Data/20240328_145247.jpg\n",
      "Extracting data for: 20240328_145247.jpg\n",
      "20240328_145247.jpg scored: 7\n",
      "Processing image: data/Sample_Data/20240328_145458.jpg\n",
      "Extracting data for: 20240328_145458.jpg\n",
      "20240328_145458.jpg scored: 4\n",
      "Processing image: data/Sample_Data/20240328_145719.jpg\n",
      "Extracting data for: 20240328_145719.jpg\n",
      "20240328_145719.jpg scored: 5\n",
      "Processing image: data/Sample_Data/20240328_145942.jpg\n",
      "Extracting data for: 20240328_145942.jpg\n",
      "20240328_145942.jpg scored: 5\n",
      "Processing image: data/Sample_Data/20240328_150204.jpg\n",
      "Extracting data for: 20240328_150204.jpg\n",
      "20240328_150204.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_150223.jpg\n",
      "Extracting data for: 20240328_150223.jpg\n",
      "20240328_150223.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_150414.jpg\n",
      "Extracting data for: 20240328_150414.jpg\n",
      "20240328_150414.jpg scored: 5\n",
      "Processing image: data/Sample_Data/20240328_150636.jpg\n",
      "Extracting data for: 20240328_150636.jpg\n",
      "20240328_150636.jpg scored: 7\n",
      "Processing image: data/Sample_Data/20240328_150651.jpg\n",
      "Extracting data for: 20240328_150651.jpg\n",
      "20240328_150651.jpg scored: 9\n",
      "Processing image: data/Sample_Data/20240328_150813.jpg\n",
      "Extracting data for: 20240328_150813.jpg\n",
      "20240328_150813.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_150931.jpg\n",
      "Extracting data for: 20240328_150931.jpg\n",
      "20240328_150931.jpg scored: 5\n",
      "Processing image: data/Sample_Data/20240328_151020.jpg\n",
      "Extracting data for: 20240328_151020.jpg\n",
      "20240328_151020.jpg scored: 3\n",
      "Processing image: data/Sample_Data/20240328_151033.jpg\n",
      "Extracting data for: 20240328_151033.jpg\n",
      "20240328_151033.jpg scored: 4\n",
      "Processing image: data/Sample_Data/20240328_151046.jpg\n",
      "Extracting data for: 20240328_151046.jpg\n",
      "20240328_151046.jpg scored: 5\n",
      "Processing image: data/Sample_Data/20240328_151122.jpg\n",
      "Extracting data for: 20240328_151122.jpg\n",
      "20240328_151122.jpg scored: 4\n",
      "Processing image: data/Sample_Data/20240328_151238.jpg\n",
      "Extracting data for: 20240328_151238.jpg\n",
      "20240328_151238.jpg scored: 2\n",
      "Processing image: data/Sample_Data/20240328_151304.jpg\n",
      "Extracting data for: 20240328_151304.jpg\n",
      "20240328_151304.jpg scored: 3\n",
      "Processing image: data/Sample_Data/20240328_151342.jpg\n",
      "Extracting data for: 20240328_151342.jpg\n",
      "20240328_151342.jpg scored: 2\n",
      "Processing image: data/Sample_Data/20240328_152314.jpg\n",
      "Extracting data for: 20240328_152314.jpg\n",
      "20240328_152314.jpg scored: 7\n",
      "Processing image: data/Sample_Data/20240328_152325.jpg\n",
      "Extracting data for: 20240328_152325.jpg\n",
      "20240328_152325.jpg scored: 1\n",
      "Processing image: data/Sample_Data/20240328_152349.jpg\n",
      "Extracting data for: 20240328_152349.jpg\n",
      "20240328_152349.jpg scored: 3\n",
      "Processing image: data/Sample_Data/20240328_152408.jpg\n",
      "Extracting data for: 20240328_152408.jpg\n",
      "20240328_152408.jpg scored: 7\n",
      "Processing image: data/Sample_Data/20240328_152515.jpg\n",
      "Extracting data for: 20240328_152515.jpg\n",
      "20240328_152515.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_153036.jpg\n",
      "Extracting data for: 20240328_153036.jpg\n",
      "20240328_153036.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_153252.jpg\n",
      "Extracting data for: 20240328_153252.jpg\n",
      "20240328_153252.jpg scored: 3\n",
      "Processing image: data/Sample_Data/20240328_153306.jpg\n",
      "Extracting data for: 20240328_153306.jpg\n",
      "20240328_153306.jpg scored: 7\n",
      "Processing image: data/Sample_Data/20240328_153316.jpg\n",
      "Extracting data for: 20240328_153316.jpg\n",
      "20240328_153316.jpg scored: 4\n",
      "Processing image: data/Sample_Data/20240328_153338.jpg\n",
      "Extracting data for: 20240328_153338.jpg\n",
      "20240328_153338.jpg scored: 4\n",
      "Processing image: data/Sample_Data/20240328_153349.jpg\n",
      "Extracting data for: 20240328_153349.jpg\n",
      "20240328_153349.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_153433.jpg\n",
      "Extracting data for: 20240328_153433.jpg\n",
      "20240328_153433.jpg scored: 3\n",
      "Processing image: data/Sample_Data/20240328_153456.jpg\n",
      "Extracting data for: 20240328_153456.jpg\n",
      "20240328_153456.jpg scored: 2\n",
      "Processing image: data/Sample_Data/20240328_153522.jpg\n",
      "Extracting data for: 20240328_153522.jpg\n",
      "20240328_153522.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_153543.jpg\n",
      "Extracting data for: 20240328_153543.jpg\n",
      "20240328_153543.jpg scored: 5\n",
      "Processing image: data/Sample_Data/20240328_153807.jpg\n",
      "Extracting data for: 20240328_153807.jpg\n",
      "20240328_153807.jpg scored: 0\n",
      "Processing image: data/Sample_Data/20240328_154250.jpg\n",
      "Extracting data for: 20240328_154250.jpg\n",
      "20240328_154250.jpg scored: 7\n",
      "Processing image: data/Sample_Data/20240328_154321.jpg\n",
      "Extracting data for: 20240328_154321.jpg\n",
      "20240328_154321.jpg scored: 4\n",
      "Processing image: data/Sample_Data/20240328_154601.jpg\n",
      "Extracting data for: 20240328_154601.jpg\n",
      "20240328_154601.jpg scored: 8\n",
      "Processing image: data/Sample_Data/20240328_154610.jpg\n",
      "Extracting data for: 20240328_154610.jpg\n",
      "20240328_154610.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_154830.jpg\n",
      "Extracting data for: 20240328_154830.jpg\n",
      "20240328_154830.jpg scored: 7\n",
      "Processing image: data/Sample_Data/20240328_155246.jpg\n",
      "Extracting data for: 20240328_155246.jpg\n",
      "20240328_155246.jpg scored: 2\n",
      "Processing image: data/Sample_Data/20240328_155417.jpg\n",
      "Extracting data for: 20240328_155417.jpg\n",
      "20240328_155417.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_155745.jpg\n",
      "Extracting data for: 20240328_155745.jpg\n",
      "20240328_155745.jpg scored: 3\n",
      "Processing image: data/Sample_Data/20240328_155758.jpg\n",
      "Extracting data for: 20240328_155758.jpg\n",
      "20240328_155758.jpg scored: 8\n",
      "Processing image: data/Sample_Data/20240328_155818.jpg\n",
      "Extracting data for: 20240328_155818.jpg\n",
      "20240328_155818.jpg scored: 4\n",
      "Processing image: data/Sample_Data/20240328_155852.jpg\n",
      "Extracting data for: 20240328_155852.jpg\n",
      "20240328_155852.jpg scored: 6\n",
      "Processing image: data/Sample_Data/20240328_160040.jpg\n",
      "Extracting data for: 20240328_160040.jpg\n",
      "20240328_160040.jpg scored: 4\n",
      "\n",
      "Errored images:\n"
     ]
    }
   ],
   "source": [
    "# Define the path to the folder containing images and the CSV file for marks\n",
    "folder_path = pathlib.Path(\"data/Sample_Data\")\n",
    "marks_csv = folder_path / \"marks.csv\"\n",
    "\n",
    "# Write the header to the marks CSV file if it doesn't exist\n",
    "if not marks_csv.exists():\n",
    "    with open(marks_csv, \"w\") as f:\n",
    "        f.write(\"image_name,pred_marks\\n\")\n",
    "\n",
    "# Read the existing marks CSV file into a DataFrame\n",
    "marks_df = pd.read_csv(marks_csv)\n",
    "\n",
    "# List to hold paths of images that resulted in errors during processing\n",
    "errored_images = []\n",
    "\n",
    "# Iterate through each image in the folder\n",
    "for image_path in folder_path.glob(\"*.jpg\"):\n",
    "    image_name = image_path.name\n",
    "    image_path_str = str(image_path).replace(\"\\\\\", \"/\")\n",
    "    print(f\"Processing image: {image_path_str}\")\n",
    "\n",
    "    total_marks = 0\n",
    "    try:\n",
    "        # Perform extraction and classification on the image\n",
    "        predictions = perform_extraction_and_classify_image(image_path_str, model)\n",
    "        \n",
    "        # Compare predictions with correct answers and calculate total marks\n",
    "        for i in range(min(len(predictions), len(ANSWERS))):  # Use min() to avoid index out of range error\n",
    "            if predictions[i].lower() == str(ANSWERS[i]).lower():  # Compare ignoring case\n",
    "                total_marks += 1\n",
    "        \n",
    "        print(f\"{image_name} scored: {total_marks}\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        # Record the image path that caused an error\n",
    "        errored_images.append(image_path_str)\n",
    "        print(f\"ERROR: {image_path_str} - {str(e)}\")\n",
    "        \n",
    "    # Append the results (image name and predicted marks) to the marks DataFrame\n",
    "    marks_df = pd.concat([marks_df, pd.DataFrame({\"image_name\": [image_name], \"pred_marks\": [total_marks]})], ignore_index=True)\n",
    "\n",
    "# Write the updated marks DataFrame back to the CSV file\n",
    "marks_df.to_csv(marks_csv, index=False)\n",
    "\n",
    "# Print list of images that caused errors during processing\n",
    "if len(errored_images) >0:\n",
    "    print(\"\\nErrored images: \", end=\"\")\n",
    "    for err_image in errored_images:\n",
    "        print(err_image, end=\", \")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
